# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rq9MyLNW1I_emFwQEEkQGZIbMpr9fkC5
"""

import argparse
import logging
import os
import re
import sys
from typing import List

import joblib
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Optional: use nltk for lemmatization
USE_NLTK = True
if USE_NLTK:
    try:
        import nltk
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer

        # Ensure required data is available
        try:
            _ = stopwords.words("english")
            _ = WordNetLemmatizer()
        except LookupError:
            nltk.download("punkt", quiet=True)
            nltk.download("wordnet", quiet=True)
            nltk.download("omw-1.4", quiet=True)
            nltk.download("stopwords", quiet=True)
            _ = stopwords.words("english")
            _ = WordNetLemmatizer()
    except Exception:
        USE_NLTK = False

# Fallback stopwords
from sklearn.feature_extraction import text as sklearn_text

DEFAULT_STOPWORDS = set(sklearn_text.ENGLISH_STOP_WORDS)
if USE_NLTK:
    try:
        DEFAULT_STOPWORDS |= set(stopwords.words("english"))
    except Exception:
        pass

logger = logging.getLogger("analysis")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)


def clean_text(text: str, lemmatize: bool = True) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()
    # Remove URLs and emails
    text = re.sub(r"http\S+|www\.\S+", " ", text)
    text = re.sub(r"\S+@\S+", " ", text)
    # Remove non letters (keep spaces)
    text = re.sub(r"[^a-z\s]", " ", text)
    # Collapse whitespace
    text = re.sub(r"\s+", " ", text).strip()
    # Remove stopwords and optionally lemmatize
    tokens = [t for t in text.split() if t not in DEFAULT_STOPWORDS and len(t) > 1]
    if lemmatize and USE_NLTK:
        try:
            lemmatizer = WordNetLemmatizer()
            tokens = [lemmatizer.lemmatize(t) for t in tokens]
        except Exception:
            pass
    return " ".join(tokens)


def build_corpus(df: pd.DataFrame, text_columns: List[str]) -> pd.Series:
    present_cols = [c for c in text_columns if c in df.columns]
    if not present_cols:
        raise ValueError(f"None of the specified text columns are present. Available columns: {df.columns.tolist()}")
    # Fill NaN with empty strings
    df_loc = df.copy()
    df_loc[present_cols] = df_loc[present_cols].fillna("")
    corpus = df_loc[present_cols].astype(str).agg(" ".join, axis=1)
    return corpus


def main(args):
    if not os.path.isfile(args.input):
        logger.error("Input file not found: %s", args.input)
        sys.exit(1)

    logger.info("Loading CSV: %s", args.input)
    df = pd.read_csv(args.input, low_memory=False)

    logger.info("Building corpus from columns: %s", args.text_columns)
    try:
        corpus_raw = build_corpus(df, args.text_columns)
    except ValueError as e:
        logger.error(str(e))
        sys.exit(1)

    logger.info("Cleaning text (lemmatize=%s)...", args.lemmatize)
    corpus_clean = corpus_raw.apply(lambda t: clean_text(t, lemmatize=args.lemmatize))
    non_empty_mask = corpus_clean.str.strip().astype(bool)
    if non_empty_mask.sum() == 0:
        logger.error("No non-empty text after cleaning. Exiting.")
        sys.exit(1)

    df = df.loc[non_empty_mask].copy()
    df["clean_corpus"] = corpus_clean.loc[non_empty_mask].values

    # Vectorization for LDA: CountVectorizer
    logger.info("Vectorizing for LDA: max_features=%d, ngram_range=%s", args.count_features, args.ngram_range)
    count_vect = CountVectorizer(max_features=args.count_features, ngram_range=tuple(args.ngram_range))
    counts = count_vect.fit_transform(df["clean_corpus"])

    # Fit LDA
    logger.info("Fitting LDA with %d topics", args.lda_topics)
    lda = LatentDirichletAllocation(n_components=args.lda_topics, random_state=42, learning_method="batch", max_iter=10)
    lda.fit(counts)

    # Show top words per topic
    feature_names = count_vect.get_feature_names_out()
    logger.info("--- TOP WORDS PER LDA TOPIC ---")
    n_top_words = args.top_words
    for idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        logger.info("Topic %d: %s", idx + 1, ", ".join(top_words))

    # Vectorization for clustering: TF-IDF
    logger.info("Vectorizing for clustering (TF-IDF): max_features=%d, ngram_range=%s", args.tfidf_features, args.ngram_range)
    tfidf = TfidfVectorizer(max_features=args.tfidf_features, ngram_range=tuple(args.ngram_range), stop_words=None)
    tfidf_matrix = tfidf.fit_transform(df["clean_corpus"])

    # KMeans clustering
    logger.info("Fitting KMeans with %d clusters", args.clusters)
    kmeans = KMeans(n_clusters=args.clusters, random_state=42, n_init=10)
    df["cluster"] = kmeans.fit_predict(tfidf_matrix)

    logger.info("\n--- K-MEANS CLUSTER SIZES ---")
    cluster_counts = df["cluster"].value_counts().sort_index()
    for c, cnt in cluster_counts.items():
        logger.info("Cluster %s: %d rows", c, cnt)

    # Save results
    logger.info("Saving results to %s", args.output)
    df.to_csv(args.output, index=False)

    # Optionally save models/vectorizers
    if args.save_models:
        base = os.path.splitext(args.output)[0]
        logger.info("Saving vectorizers and models to %s_*.joblib", base)
        joblib.dump(count_vect, base + "_count_vectorizer.joblib")
        joblib.dump(tfidf, base + "_tfidf_vectorizer.joblib")
        joblib.dump(lda, base + "_lda_model.joblib")
        joblib.dump(kmeans, base + "_kmeans_model.joblib")

    logger.info("Success! Results saved to %s", args.output)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="NLP analysis for cybersecurity dataset")
    parser.add_argument("--input", "-i", required=True, help="Input CSV file path")
    parser.add_argument(
        "--text-columns",
        "-c",
        nargs="?",
        default=["Attack Type", "Target Industry", "Attack Source", "Security Vulnerability Type"],
        help="Columns to combine into the corpus (default: 4 columns used previously)",
    )
    parser.add_argument("--output", "-o", default="Analysis_Results.csv", help="Output CSV path")
    parser.add_argument("--tfidf-features", type=int, default=1000, help="Max features for TF-IDF vectorizer")
    parser.add_argument("--count-features", type=int, default=1000, help="Max features for CountVectorizer (LDA)")
    parser.add_argument("--lda-topics", type=int, default=6, help="Number of LDA topics")
    parser.add_argument("--clusters", type=int, default=6, help="Number of KMeans clusters")
    parser.add_argument("--top-words", type=int, default=10, help="Top words to display per topic")
    parser.add_argument("--save-models", action="store_true", help="Save vectorizers and models as joblib files")
    parser.add_argument("--lemmatize", action="store_true", help="Apply lemmatization (requires NLTK)")
    parser.add_argument("--ngram-range", nargs=2, type=int, default=[1, 2], help="Ngram range (two ints, e.g. 1 2)")

    # For Colab execution, provide mock arguments to argparse
    args = parser.parse_args(["--input", "Global_Cybersecurity_Threats_2015-2024 (1).csv"])

    main(args)

import argparse
import logging
import os
import re
import sys
from typing import List

import joblib
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Optional: use nltk for lemmatization
USE_NLTK = True
if USE_NLTK:
    try:
        import nltk
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer

        # Ensure required data is available
        try:
            _ = stopwords.words("english")
            _ = WordNetLemmatizer()
        except LookupError:
            nltk.download("punkt", quiet=True)
            nltk.download("wordnet", quiet=True)
            nltk.download("omw-1.4", quiet=True)
            nltk.download("stopwords", quiet=True)
            _ = stopwords.words("english")
            _ = WordNetLemmatizer()
    except Exception:
        USE_NLTK = False

# Fallback stopwords
from sklearn.feature_extraction import text as sklearn_text

DEFAULT_STOPWORDS = set(sklearn_text.ENGLISH_STOP_WORDS)
if USE_NLTK:
    try:
        DEFAULT_STOPWORDS |= set(stopwords.words("english"))
    except Exception:
        pass

logger = logging.getLogger("analysis")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)


def clean_text(text: str, lemmatize: bool = True) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()
    # Remove URLs and emails
    text = re.sub(r"http\S+|www\.\S+", " ", text)
    text = re.sub(r"\S+@\S+", " ", text)
    # Remove non letters (keep spaces)
    text = re.sub(r"[^a-z\s]", " ", text)
    # Collapse whitespace
    text = re.sub(r"\s+", " ", text).strip()
    # Remove stopwords and optionally lemmatize
    tokens = [t for t in text.split() if t not in DEFAULT_STOPWORDS and len(t) > 1]
    if lemmatize and USE_NLTK:
        try:
            lemmatizer = WordNetLemmatizer()
            tokens = [lemmatizer.lemmatize(t) for t in tokens]
        except Exception:
            pass
    return " ".join(tokens)


def build_corpus(df: pd.DataFrame, text_columns: List[str]) -> pd.Series:
    present_cols = [c for c in text_columns if c in df.columns]
    if not present_cols:
        raise ValueError(f"None of the specified text columns are present. Available columns: {df.columns.tolist()}")
    # Fill NaN with empty strings
    df_loc = df.copy()
    df_loc[present_cols] = df_loc[present_cols].fillna("")
    corpus = df_loc[present_cols].astype(str).agg(" ".join, axis=1)
    return corpus


def main(args):
    if not os.path.isfile(args.input):
        logger.error("Input file not found: %s", args.input)
        sys.exit(1)

    logger.info("Loading CSV: %s", args.input)
    df = pd.read_csv(args.input, low_memory=False)

    logger.info("Building corpus from columns: %s", args.text_columns)
    try:
        corpus_raw = build_corpus(df, args.text_columns)
    except ValueError as e:
        logger.error(str(e))
        sys.exit(1)

    logger.info("Cleaning text (lemmatize=%s)...", args.lemmatize)
    corpus_clean = corpus_raw.apply(lambda t: clean_text(t, lemmatize=args.lemmatize))
    non_empty_mask = corpus_clean.str.strip().astype(bool)
    if non_empty_mask.sum() == 0:
        logger.error("No non-empty text after cleaning. Exiting.")
        sys.exit(1)

    df = df.loc[non_empty_mask].copy()
    df["clean_corpus"] = corpus_clean.loc[non_empty_mask].values

    # Vectorization for LDA: CountVectorizer
    logger.info("Vectorizing for LDA: max_features=%d, ngram_range=%s", args.count_features, args.ngram_range)
    count_vect = CountVectorizer(max_features=args.count_features, ngram_range=tuple(args.ngram_range))
    counts = count_vect.fit_transform(df["clean_corpus"])

    # Fit LDA
    logger.info("Fitting LDA with %d topics", args.lda_topics)
    lda = LatentDirichletAllocation(n_components=args.lda_topics, random_state=42, learning_method="batch", max_iter=10)
    lda.fit(counts)

    # Show top words per topic
    feature_names = count_vect.get_feature_names_out()
    logger.info("--- TOP WORDS PER LDA TOPIC ---")
    n_top_words = args.top_words
    for idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        logger.info("Topic %d: %s", idx + 1, ", ".join(top_words))

    # Vectorization for clustering: TF-IDF
    logger.info("Vectorizing for clustering (TF-IDF): max_features=%d, ngram_range=%s", args.tfidf_features, args.ngram_range)
    tfidf = TfidfVectorizer(max_features=args.tfidf_features, ngram_range=tuple(args.ngram_range), stop_words=None)
    tfidf_matrix = tfidf.fit_transform(df["clean_corpus"])

    # KMeans clustering
    logger.info("Fitting KMeans with %d clusters", args.clusters)
    kmeans = KMeans(n_clusters=args.clusters, random_state=42, n_init=10)
    df["cluster"] = kmeans.fit_predict(tfidf_matrix)

    logger.info("\n--- K-MEANS CLUSTER SIZES ---")
    cluster_counts = df["cluster"].value_counts().sort_index()
    for c, cnt in cluster_counts.items():
        logger.info("Cluster %s: %d rows", c, cnt)

    # Save results
    logger.info("Saving results to %s", args.output)
    df.to_csv(args.output, index=False)

    # Optionally save models/vectorizers
    if args.save_models:
        base = os.path.splitext(args.output)[0]
        logger.info("Saving vectorizers and models to %s_*.joblib", base)
        joblib.dump(count_vect, base + "_count_vectorizer.joblib")
        joblib.dump(tfidf, base + "_tfidf_vectorizer.joblib")
        joblib.dump(lda, base + "_lda_model.joblib")
        joblib.dump(kmeans, base + "_kmeans_model.joblib")

    logger.info("Success! Results saved to %s", args.output)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="NLP analysis for cybersecurity dataset")
    parser.add_argument("--input", "-i", required=True, help="Input CSV file path")
    parser.add_argument(
        "--text-columns",
        "-c",
        nargs="?",
        default=["Attack Type", "Target Industry", "Attack Source", "Security Vulnerability Type"],
        help="Columns to combine into the corpus (default: 4 columns used previously)",
    )
    parser.add_argument("--output", "-o", default="Analysis_Results.csv", help="Output CSV path")
    parser.add_argument("--tfidf-features", type=int, default=1000, help="Max features for TF-IDF vectorizer")
    parser.add_argument("--count-features", type=int, default=1000, help="Max features for CountVectorizer (LDA)")
    parser.add_argument("--lda-topics", type=int, default=6, help="Number of LDA topics")
    parser.add_argument("--clusters", type=int, default=6, help="Number of KMeans clusters")
    parser.add_argument("--top-words", type=int, default=10, help="Top words to display per topic")
    parser.add_argument("--save-models", action="store_true", help="Save vectorizers and models as joblib files")
    parser.add_argument("--lemmatize", action="store_true", help="Apply lemmatization (requires NLTK)")
    parser.add_argument("--ngram-range", nargs=2, type=int, default=[1, 2], help="Ngram range (two ints, e.g. 1 2)")

    # For Colab execution, provide mock arguments to argparse
    args = parser.parse_args(["--input", "Global_Cybersecurity_Threats_2015-2024 (1).csv"])

    main(args)

import pandas as pd

analysis_df = pd.read_csv('Analysis_Results.csv')
display(analysis_df.head())

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming analysis_df is already loaded and contains 'clean_corpus' and 'cluster'
# Re-vectorize clean_corpus to get tfidf_matrix
# Use parameters consistent with the original script if possible, or adjust as needed
tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), stop_words=None)
tfidf_matrix = tfidf.fit_transform(analysis_df["clean_corpus"])

# 1. Reduce the data to 2 dimensions for the graph
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(tfidf_matrix.toarray())

# 2. Plot the clusters
plt.figure(figsize=(10, 7))
scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=analysis_df['cluster'], cmap='viridis', alpha=0.5)
plt.title('Cybersecurity Threat Clusters (Phase 2)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(scatter, label='Cluster ID')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Ensure analysis_df is loaded
analysis_df = pd.read_csv('Analysis_Results.csv')

# Calculate the count of each cluster per country
country_cluster_counts = analysis_df.groupby(['Country', 'cluster']).size().unstack(fill_value=0)

# Plotting a stacked bar chart
fig = plt.figure(figsize=(14, 8))
country_cluster_counts.plot(kind='bar', stacked=True, cmap='viridis', figsize=(14, 8))
plt.title('Distribution of Cybersecurity Threat Clusters by Country')
plt.xlabel('Country')
plt.ylabel('Number of Incidents')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Cluster')
plt.tight_layout()
plt.show()

import pandas as pd

# Load the specified dataset
try:
    analysis_df = pd.read_csv('Global_Cybersecurity_Threats_2015-2024.csv')
except FileNotFoundError:
    print("Global_Cybersecurity_Threats_2015-2024.csv not found. Please ensure the file exists and is in the correct directory.")
    # Exit or handle error if the primary input file is missing
    exit()

# Define the columns to be combined
text_columns_to_combine = ['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source']

# Check if all specified columns exist in analysis_df
missing_cols = [col for col in text_columns_to_combine if col not in analysis_df.columns]
if missing_cols:
    print(f"Warning: The following columns are missing from analysis_df and will be skipped: {missing_cols}")
    text_columns_to_combine = [col for col in text_columns_to_combine if col not in missing_cols]

# Combine the text from the specified columns into a new column called 'custom_corpus'
# Fill NaN with empty strings and convert to string type before concatenation
analysis_df['custom_corpus'] = analysis_df[text_columns_to_combine].fillna('').astype(str).agg(' '.join, axis=1)

# Display the first few rows with the new custom_corpus column
print("DataFrame with 'custom_corpus' column:")
display(analysis_df[['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source', 'custom_corpus']].head())

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

# Download necessary NLTK data if not already present
try:
    _ = stopwords.words('english')
    _ = WordNetLemmatizer()
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
    nltk.download('stopwords', quiet=True)

def clean_text(text):
    # Lowercasing
    text = text.lower()
    # Remove non-alphabetic characters and replace with space
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize (split into words)
    words = text.split()

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    # Join back into a string
    return ' '.join(words)

# Apply the cleaning function to the 'custom_corpus' column
analysis_df['clean_custom_corpus'] = analysis_df['custom_corpus'].apply(clean_text)

# Display the first few rows of the DataFrame, focusing on the 'clean_custom_corpus' column
print("DataFrame with 'clean_custom_corpus' column:")
display(analysis_df[['custom_corpus', 'clean_custom_corpus']].head())

display(analysis_df[['Attack Type', 'clean_custom_corpus']].head())

import argparse
import logging
import os
import re
import sys
from typing import List

import joblib
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Optional: use nltk for lemmatization
USE_NLTK = True
if USE_NLTK:
    try:
        import nltk
        from nltk.corpus import stopwords
        from nltk.stem import WordNetLemmatizer

        # Ensure required data is available
        try:
            _ = stopwords.words("english")
            _ = WordNetLemmatizer()
        except LookupError:
            nltk.download("punkt", quiet=True)
            nltk.download("wordnet", quiet=True)
            nltk.download("omw-1.4", quiet=True)
            nltk.download("stopwords", quiet=True)
            _ = stopwords.words("english")
            _ = WordNetLemmatizer()
    except Exception:
        USE_NLTK = False

# Fallback stopwords
from sklearn.feature_extraction import text as sklearn_text

DEFAULT_STOPWORDS = set(sklearn_text.ENGLISH_STOP_WORDS)
if USE_NLTK:
    try:
        DEFAULT_STOPWORDS |= set(stopwords.words("english"))
    except Exception:
        pass

logger = logging.getLogger("analysis")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)


def clean_text(text: str, lemmatize: bool = True) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()
    # Remove URLs and emails
    text = re.sub(r"http\S+|www\.\S+", " ", text)
    text = re.sub(r"\S+@\S+", " ", text)
    # Remove non letters (keep spaces)
    text = re.sub(r"[^a-z\s]", " ", text)
    # Collapse whitespace
    text = re.sub(r"\s+", " ", text).strip()
    # Remove stopwords and optionally lemmatize
    tokens = [t for t in text.split() if t not in DEFAULT_STOPWORDS and len(t) > 1]
    if lemmatize and USE_NLTK:
        try:
            lemmatizer = WordNetLemmatizer()
            tokens = [lemmatizer.lemmatize(t) for t in tokens]
        except Exception:
            pass
    return " ".join(tokens)


def build_corpus(df: pd.DataFrame, text_columns: List[str]) -> pd.Series:
    present_cols = [c for c in text_columns if c in df.columns]
    if not present_cols:
        raise ValueError(f"None of the specified text columns are present. Available columns: {df.columns.tolist()}")
    # Fill NaN with empty strings
    df_loc = df.copy()
    df_loc[present_cols] = df_loc[present_cols].fillna("")
    corpus = df_loc[present_cols].astype(str).agg(" ".join, axis=1)
    return corpus


def main(args):
    if not os.path.isfile(args.input):
        logger.error("Input file not found: %s", args.input)
        sys.exit(1)

    logger.info("Loading CSV: %s", args.input)
    df = pd.read_csv(args.input, low_memory=False)

    logger.info("Building corpus from columns: %s", args.text_columns)
    try:
        corpus_raw = build_corpus(df, args.text_columns)
    except ValueError as e:
        logger.error(str(e))
        sys.exit(1)

    logger.info("Cleaning text (lemmatize=%s)...", args.lemmatize)
    corpus_clean = corpus_raw.apply(lambda t: clean_text(t, lemmatize=args.lemmatize))
    non_empty_mask = corpus_clean.str.strip().astype(bool)
    if non_empty_mask.sum() == 0:
        logger.error("No non-empty text after cleaning. Exiting.")
        sys.exit(1)

    df = df.loc[non_empty_mask].copy()
    df["clean_corpus"] = corpus_clean.loc[non_empty_mask].values

    # Vectorization for LDA: CountVectorizer
    logger.info("Vectorizing for LDA: max_features=%d, ngram_range=%s", args.count_features, args.ngram_range)
    count_vect = CountVectorizer(max_features=args.count_features, ngram_range=tuple(args.ngram_range))
    counts = count_vect.fit_transform(df["clean_corpus"])

    # Fit LDA
    logger.info("Fitting LDA with %d topics", args.lda_topics)
    lda = LatentDirichletAllocation(n_components=args.lda_topics, random_state=42, learning_method="batch", max_iter=10)
    lda.fit(counts)

    # Show top words per topic
    feature_names = count_vect.get_feature_names_out()
    logger.info("--- TOP WORDS PER LDA TOPIC ---")
    n_top_words = args.top_words
    for idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[:-n_top_words - 1:-1]
        top_words = [feature_names[i] for i in top_indices]
        logger.info("Topic %d: %s", idx + 1, ", ".join(top_words))

    # Vectorization for clustering: TF-IDF
    logger.info("Vectorizing for clustering (TF-IDF): max_features=%d, ngram_range=%s", args.tfidf_features, args.ngram_range)
    tfidf = TfidfVectorizer(max_features=args.tfidf_features, ngram_range=tuple(args.ngram_range), stop_words=None)
    tfidf_matrix = tfidf.fit_transform(df["clean_corpus"])

    # KMeans clustering
    logger.info("Fitting KMeans with %d clusters", args.clusters)
    kmeans = KMeans(n_clusters=args.clusters, random_state=42, n_init=10)
    df["cluster"] = kmeans.fit_predict(tfidf_matrix)

    logger.info("\n--- K-MEANS CLUSTER SIZES ---")
    cluster_counts = df["cluster"].value_counts().sort_index()
    for c, cnt in cluster_counts.items():
        logger.info("Cluster %s: %d rows", c, cnt)

    # Save results
    logger.info("Saving results to %s", args.output)
    df.to_csv(args.output, index=False)

    # Optionally save models/vectorizers
    if args.save_models:
        base = os.path.splitext(args.output)[0]
        logger.info("Saving vectorizers and models to %s_*.joblib", base)
        joblib.dump(count_vect, base + "_count_vectorizer.joblib")
        joblib.dump(tfidf, base + "_tfidf_vectorizer.joblib")
        joblib.dump(lda, base + "_lda_model.joblib")
        joblib.dump(kmeans, base + "_kmeans_model.joblib")

    logger.info("Success! Results saved to %s", args.output)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="NLP analysis for cybersecurity dataset")
    parser.add_argument("--input", "-i", required=True, help="Input CSV file path")
    parser.add_argument(
        "--text-columns",
        "-c",
        nargs="?",
        default=["Attack Type", "Target Industry", "Attack Source", "Security Vulnerability Type"],
        help="Columns to combine into the corpus (default: 4 columns used previously)",
    )
    parser.add_argument("--output", "-o", default="Analysis_Results.csv", help="Output CSV path")
    parser.add_argument("--tfidf-features", type=int, default=1000, help="Max features for TF-IDF vectorizer")
    parser.add_argument("--count-features", type=int, default=1000, help="Max features for CountVectorizer (LDA)")
    parser.add_argument("--lda-topics", type=int, default=6, help="Number of LDA topics")
    parser.add_argument("--clusters", type=int, default=6, help="Number of KMeans clusters")
    parser.add_argument("--top-words", type=int, default=10, help="Top words to display per topic")
    parser.add_argument("--save-models", action="store_true", help="Save vectorizers and models as joblib files")
    parser.add_argument("--lemmatize", action="store_true", help="Apply lemmatization (requires NLTK)")
    parser.add_argument("--ngram-range", nargs=2, type=int, default=[1, 2], help="Ngram range (two ints, e.g. 1 2)")

    # For Colab execution, provide mock arguments to argparse
    args = parser.parse_args(["--input", "Global_Cybersecurity_Threats_2015-2024 (1).csv"])

    main(args)

import pandas as pd

analysis_df = pd.read_csv('Analysis_Results.csv')
display(analysis_df.head())

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming analysis_df is already loaded and contains 'clean_corpus' and 'cluster'
# Re-vectorize clean_corpus to get tfidf_matrix
# Use parameters consistent with the original script if possible, or adjust as needed
tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), stop_words=None)
tfidf_matrix = tfidf.fit_transform(analysis_df["clean_corpus"])

# 1. Reduce the data to 2 dimensions for the graph
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(tfidf_matrix.toarray())

# 2. Plot the clusters
plt.figure(figsize=(10, 7))
scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=analysis_df['cluster'], cmap='viridis', alpha=0.5)
plt.title('Cybersecurity Threat Clusters (Phase 2)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(scatter, label='Cluster ID')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Ensure analysis_df is loaded
analysis_df = pd.read_csv('Analysis_Results.csv')

# Calculate the count of each cluster per country
country_cluster_counts = analysis_df.groupby(['Country', 'cluster']).size().unstack(fill_value=0)

# Plotting a stacked bar chart
fig = plt.figure(figsize=(14, 8))
country_cluster_counts.plot(kind='bar', stacked=True, cmap='viridis', figsize=(14, 8))
plt.title('Distribution of Cybersecurity Threat Clusters by Country')
plt.xlabel('Country')
plt.ylabel('Number of Incidents')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Cluster')
plt.tight_layout()
plt.show()

import pandas as pd

# Load the specified dataset
try:
    analysis_df = pd.read_csv('Global_Cybersecurity_Threats_2015-2024.csv')
except FileNotFoundError:
    print("Global_Cybersecurity_Threats_2015-2024.csv not found. Please ensure the file exists and is in the correct directory.")
    # Exit or handle error if the primary input file is missing
    exit()

# Define the columns to be combined
text_columns_to_combine = ['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source']

# Check if all specified columns exist in analysis_df
missing_cols = [col for col in text_columns_to_combine if col not in analysis_df.columns]
if missing_cols:
    print(f"Warning: The following columns are missing from analysis_df and will be skipped: {missing_cols}")
    text_columns_to_combine = [col for col in text_columns_to_combine if col not in missing_cols]

# Combine the text from the specified columns into a new column called 'custom_corpus'
# Fill NaN with empty strings and convert to string type before concatenation
analysis_df['custom_corpus'] = analysis_df[text_columns_to_combine].fillna('').astype(str).agg(' '.join, axis=1)

# Display the first few rows with the new custom_corpus column
print("DataFrame with 'custom_corpus' column:")
display(analysis_df[['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source', 'custom_corpus']].head())

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

# Download necessary NLTK data if not already present
try:
    _ = stopwords.words('english')
    _ = WordNetLemmatizer()
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
    nltk.download('stopwords', quiet=True)

def clean_text(text):
    # Lowercasing
    text = text.lower()
    # Remove non-alphabetic characters and replace with space
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize (split into words)
    words = text.split()

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    # Join back into a string
    return ' '.join(words)

# Apply the cleaning function to the 'custom_corpus' column
analysis_df['clean_custom_corpus'] = analysis_df['custom_corpus'].apply(clean_text)

# Display the first few rows of the DataFrame, focusing on the 'clean_custom_corpus' column
print("DataFrame with 'clean_custom_corpus' column:")
display(analysis_df[['custom_corpus', 'clean_custom_corpus']].head())

display(analysis_df[['Attack Type', 'clean_custom_corpus']].head())

import pandas as pd

analysis_df = pd.read_csv('Analysis_Results.csv')
display(analysis_df.head())

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer

# Assuming analysis_df is already loaded and contains 'clean_corpus' and 'cluster'
# Re-vectorize clean_corpus to get tfidf_matrix
# Use parameters consistent with the original script if possible, or adjust as needed
tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), stop_words=None)
tfidf_matrix = tfidf.fit_transform(analysis_df["clean_corpus"])

# 1. Reduce the data to 2 dimensions for the graph
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(tfidf_matrix.toarray())

# 2. Plot the clusters
plt.figure(figsize=(10, 7))
scatter = plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=analysis_df['cluster'], cmap='viridis', alpha=0.5)
plt.title('Cybersecurity Threat Clusters (Phase 2)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(scatter, label='Cluster ID')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Ensure analysis_df is loaded
analysis_df = pd.read_csv('Analysis_Results.csv')

# Calculate the count of each cluster per country
country_cluster_counts = analysis_df.groupby(['Country', 'cluster']).size().unstack(fill_value=0)

# Plotting a stacked bar chart
fig = plt.figure(figsize=(14, 8))
country_cluster_counts.plot(kind='bar', stacked=True, cmap='viridis', figsize=(14, 8))
plt.title('Distribution of Cybersecurity Threat Clusters by Country')
plt.xlabel('Country')
plt.ylabel('Number of Incidents')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Cluster')
plt.tight_layout()
plt.show()



"""# Task
Load the "Global_Cybersecurity_Threats_2015-2024 (1).csv" dataset, combine the text from 'Attack Type', 'Target Industry', 'Security Vulnerability Type', and 'Attack Source' columns into a new 'custom_corpus' column, clean this corpus by lowercasing, removing punctuation, tokenizing, removing stopwords, and lemmatizing the text, storing the result in a 'clean_custom_corpus' column, and then display the first few rows of the DataFrame, focusing on the 'clean_custom_corpus' column.

## Load Data and Combine Columns

### Subtask:
Load the 'Global_Cybersecurity_Threats_2015-2024 (1).csv' file into a pandas DataFrame. Then, combine the text from the 'Attack Type', 'Target Industry', 'Security Vulnerability Type', and 'Attack Source' columns into a new column called 'custom_corpus'. This will create a consolidated text field for further NLP analysis.

**Reasoning**:
The subtask requires combining specific text columns into a new 'custom_corpus' column. This code block will perform the concatenation after handling potential missing values.
"""

import pandas as pd

# Ensure analysis_df is loaded. If it's not already, load it.
try:
    if 'analysis_df' not in locals():
        analysis_df = pd.read_csv('Analysis_Results.csv')
except FileNotFoundError:
    print("Analysis_Results.csv not found. Please ensure the file exists and is in the correct directory.")
    # Optionally, handle this error more robustly, e.g., sys.exit(1)

# Define the columns to be combined
text_columns_to_combine = ['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source']

# Check if all specified columns exist in analysis_df
missing_cols = [col for col in text_columns_to_combine if col not in analysis_df.columns]
if missing_cols:
    print(f"Warning: The following columns are missing from analysis_df and will be skipped: {missing_cols}")
    text_columns_to_combine = [col for col in text_columns_to_combine if col not in missing_cols]

# Combine the text from the specified columns into a new column called 'custom_corpus'
# Fill NaN with empty strings and convert to string type before concatenation
analysis_df['custom_corpus'] = analysis_df[text_columns_to_combine].fillna('').astype(str).agg(' '.join, axis=1)

# Display the first few rows with the new custom_corpus column
print("DataFrame with 'custom_corpus' column:")
display(analysis_df[['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source', 'custom_corpus']].head())

"""**Reasoning**:
The previous code failed because 'Analysis_Results.csv' was not found, leading to 'analysis_df' not being defined. The task specifies loading 'Global_Cybersecurity_Threats_2015-2024 (1).csv' instead. I will correct the filename to be loaded.


"""

import pandas as pd

# Load the specified dataset
try:
    analysis_df = pd.read_csv('Global_Cybersecurity_Threats_2015-2024 (1).csv')
except FileNotFoundError:
    print("Global_Cybersecurity_Threats_2015-2024 (1).csv not found. Please ensure the file exists and is in the correct directory.")
    # Exit or handle error if the primary input file is missing
    exit()

# Define the columns to be combined
text_columns_to_combine = ['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source']

# Check if all specified columns exist in analysis_df
missing_cols = [col for col in text_columns_to_combine if col not in analysis_df.columns]
if missing_cols:
    print(f"Warning: The following columns are missing from analysis_df and will be skipped: {missing_cols}")
    text_columns_to_combine = [col for col in text_columns_to_combine if col not in missing_cols]

# Combine the text from the specified columns into a new column called 'custom_corpus'
# Fill NaN with empty strings and convert to string type before concatenation
analysis_df['custom_corpus'] = analysis_df[text_columns_to_combine].fillna('').astype(str).agg(' '.join, axis=1)

# Display the first few rows with the new custom_corpus column
print("DataFrame with 'custom_corpus' column:")
display(analysis_df[['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source', 'custom_corpus']].head())

display(analysis_df[['Attack Type', 'clean_custom_corpus']].head())

"""**Reasoning**:
The previous code failed because the specified file 'Global_Cybersecurity_Threats_2015-2024 (1).csv' was not found. The correct filename, based on the kernel state, is 'Global_Cybersecurity_Threats_2015-2024.csv'. I will correct the filename in the `pd.read_csv` call.


"""

import pandas as pd

# Load the specified dataset
try:
    analysis_df = pd.read_csv('Global_Cybersecurity_Threats_2015-2024.csv')
except FileNotFoundError:
    print("Global_Cybersecurity_Threats_2015-2024.csv not found. Please ensure the file exists and is in the correct directory.")
    # Exit or handle error if the primary input file is missing
    exit()

# Define the columns to be combined
text_columns_to_combine = ['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source']

# Check if all specified columns exist in analysis_df
missing_cols = [col for col in text_columns_to_combine if col not in analysis_df.columns]
if missing_cols:
    print(f"Warning: The following columns are missing from analysis_df and will be skipped: {missing_cols}")
    text_columns_to_combine = [col for col in text_columns_to_combine if col not in missing_cols]

# Combine the text from the specified columns into a new column called 'custom_corpus'
# Fill NaN with empty strings and convert to string type before concatenation
analysis_df['custom_corpus'] = analysis_df[text_columns_to_combine].fillna('').astype(str).agg(' '.join, axis=1)

# Display the first few rows with the new custom_corpus column
print("DataFrame with 'custom_corpus' column:")
display(analysis_df[['Attack Type', 'Target Industry', 'Security Vulnerability Type', 'Attack Source', 'custom_corpus']].head())

"""**Reasoning**:
The subtask requires cleaning the 'custom_corpus' column by lowercasing, removing punctuation, tokenizing, removing stopwords, and lemmatizing the text. This code block will import necessary NLTK components, define a cleaning function, ensure NLTK data is downloaded, and then apply the cleaning function to create the 'clean_custom_corpus' column. Finally, it will display the first few rows focusing on the new column.


"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re

# Download necessary NLTK data if not already present
try:
    _ = stopwords.words('english')
    _ = WordNetLemmatizer()
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('wordnet', quiet=True)
    nltk.download('omw-1.4', quiet=True)
    nltk.download('stopwords', quiet=True)

def clean_text(text):
    # Lowercasing
    text = text.lower()
    # Remove non-alphabetic characters and replace with space
    text = re.sub(r'[^a-z\s]', '', text)
    # Tokenize (split into words)
    words = text.split()

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    # Join back into a string
    return ' '.join(words)

# Apply the cleaning function to the 'custom_corpus' column
analysis_df['clean_custom_corpus'] = analysis_df['custom_corpus'].apply(clean_text)

# Display the first few rows of the DataFrame, focusing on the 'clean_custom_corpus' column
print("DataFrame with 'clean_custom_corpus' column:")
display(analysis_df[['custom_corpus', 'clean_custom_corpus']].head())

"""## Final Task

### Subtask:
Confirm the successful creation and display of the cleaned custom text corpus, ready for further NLP processing.

## Summary:

### Q&A
Yes, the custom text corpus was successfully created and displayed, ready for further NLP processing. The `analysis_df` DataFrame now contains both a `custom_corpus` column with the combined text and a `clean_custom_corpus` column with the processed and cleaned text.

### Data Analysis Key Findings
*   Initial attempts to load the dataset using `Analysis_Results.csv` and `Global_Cybersecurity_Threats_2015-2024 (1).csv` failed due to `FileNotFoundError`.
*   The dataset `Global_Cybersecurity_Threats_2015-2024.csv` was successfully loaded into a pandas DataFrame.
*   A new column named `custom_corpus` was created by combining text from the 'Attack Type', 'Target Industry', 'Security Vulnerability Type', and 'Attack Source' columns, with missing values handled by filling them with empty strings.
*   A `clean_custom_corpus` column was successfully generated by applying a cleaning process to the `custom_corpus` column, which included lowercasing, removing non-alphabetic characters, tokenization, stopword removal, and lemmatization. For instance, an entry like "Man-in-the-Middle IT Weak Passwords Hacker Group" was transformed into "maninthemiddle weak password hacker group".

### Insights or Next Steps
*   The `clean_custom_corpus` is now prepared for advanced Natural Language Processing (NLP) tasks such as topic modeling, sentiment analysis, or text classification, enabling deeper insights into cybersecurity threats.
*   Moving forward, consider expanding the cleaning function to handle domain-specific jargon or acronyms more effectively, which could further improve the quality of the corpus for cybersecurity-focused NLP analysis.
"""