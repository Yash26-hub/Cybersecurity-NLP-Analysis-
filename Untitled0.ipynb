{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYg95wyRZb2JH2s3LSvLUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash26-hub/Cybersecurity-NLP-Analysis-/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFIvcA5YJc5W",
        "outputId": "da636796-e59f-445a-c3c6-5020cd6d42d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:44,114 INFO Loading CSV: Global_Cybersecurity_Threats_2015-2024 (1).csv\n",
            "2026-01-27 15:57:44,114 INFO Loading CSV: Global_Cybersecurity_Threats_2015-2024 (1).csv\n",
            "2026-01-27 15:57:44,114 INFO Loading CSV: Global_Cybersecurity_Threats_2015-2024 (1).csv\n",
            "2026-01-27 15:57:44,114 INFO Loading CSV: Global_Cybersecurity_Threats_2015-2024 (1).csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Loading CSV: Global_Cybersecurity_Threats_2015-2024 (1).csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:44,132 INFO Building corpus from columns: ['Attack Type', 'Target Industry', 'Attack Source', 'Security Vulnerability Type']\n",
            "2026-01-27 15:57:44,132 INFO Building corpus from columns: ['Attack Type', 'Target Industry', 'Attack Source', 'Security Vulnerability Type']\n",
            "2026-01-27 15:57:44,132 INFO Building corpus from columns: ['Attack Type', 'Target Industry', 'Attack Source', 'Security Vulnerability Type']\n",
            "2026-01-27 15:57:44,132 INFO Building corpus from columns: ['Attack Type', 'Target Industry', 'Attack Source', 'Security Vulnerability Type']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Building corpus from columns: ['Attack Type', 'Target Industry', 'Attack Source', 'Security Vulnerability Type']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:44,161 INFO Cleaning text (lemmatize=False)...\n",
            "2026-01-27 15:57:44,161 INFO Cleaning text (lemmatize=False)...\n",
            "2026-01-27 15:57:44,161 INFO Cleaning text (lemmatize=False)...\n",
            "2026-01-27 15:57:44,161 INFO Cleaning text (lemmatize=False)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Cleaning text (lemmatize=False)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:44,204 INFO Vectorizing for LDA: max_features=1000, ngram_range=[1, 2]\n",
            "2026-01-27 15:57:44,204 INFO Vectorizing for LDA: max_features=1000, ngram_range=[1, 2]\n",
            "2026-01-27 15:57:44,204 INFO Vectorizing for LDA: max_features=1000, ngram_range=[1, 2]\n",
            "2026-01-27 15:57:44,204 INFO Vectorizing for LDA: max_features=1000, ngram_range=[1, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Vectorizing for LDA: max_features=1000, ngram_range=[1, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:44,239 INFO Fitting LDA with 6 topics\n",
            "2026-01-27 15:57:44,239 INFO Fitting LDA with 6 topics\n",
            "2026-01-27 15:57:44,239 INFO Fitting LDA with 6 topics\n",
            "2026-01-27 15:57:44,239 INFO Fitting LDA with 6 topics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Fitting LDA with 6 topics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,789 INFO --- TOP WORDS PER LDA TOPIC ---\n",
            "2026-01-27 15:57:51,789 INFO --- TOP WORDS PER LDA TOPIC ---\n",
            "2026-01-27 15:57:51,789 INFO --- TOP WORDS PER LDA TOPIC ---\n",
            "2026-01-27 15:57:51,789 INFO --- TOP WORDS PER LDA TOPIC ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:--- TOP WORDS PER LDA TOPIC ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,794 INFO Topic 1: unknown, zero, zero day, day, hacker group, hacker, group, middle, man middle, man\n",
            "2026-01-27 15:57:51,794 INFO Topic 1: unknown, zero, zero day, day, hacker group, hacker, group, middle, man middle, man\n",
            "2026-01-27 15:57:51,794 INFO Topic 1: unknown, zero, zero day, day, hacker group, hacker, group, middle, man middle, man\n",
            "2026-01-27 15:57:51,794 INFO Topic 1: unknown, zero, zero day, day, hacker group, hacker, group, middle, man middle, man\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Topic 1: unknown, zero, zero day, day, hacker group, hacker, group, middle, man middle, man\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,810 INFO Topic 2: state, nation state, nation, state zero, state weak, sql, sql injection, injection, zero, zero day\n",
            "2026-01-27 15:57:51,810 INFO Topic 2: state, nation state, nation, state zero, state weak, sql, sql injection, injection, zero, zero day\n",
            "2026-01-27 15:57:51,810 INFO Topic 2: state, nation state, nation, state zero, state weak, sql, sql injection, injection, zero, zero day\n",
            "2026-01-27 15:57:51,810 INFO Topic 2: state, nation state, nation, state zero, state weak, sql, sql injection, injection, zero, zero day\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Topic 2: state, nation state, nation, state zero, state weak, sql, sql injection, injection, zero, zero day\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,825 INFO Topic 3: engineering, social, social engineering, group social, state social, insider social, unknown social, group, hacker, hacker group\n",
            "2026-01-27 15:57:51,825 INFO Topic 3: engineering, social, social engineering, group social, state social, insider social, unknown social, group, hacker, hacker group\n",
            "2026-01-27 15:57:51,825 INFO Topic 3: engineering, social, social engineering, group social, state social, insider social, unknown social, group, hacker, hacker group\n",
            "2026-01-27 15:57:51,825 INFO Topic 3: engineering, social, social engineering, group social, state social, insider social, unknown social, group, hacker, hacker group\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Topic 3: engineering, social, social engineering, group social, state social, insider social, unknown social, group, hacker, hacker group\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,840 INFO Topic 4: retail, day, zero day, zero, insider, insider zero, middle, man middle, man, retail unknown\n",
            "2026-01-27 15:57:51,840 INFO Topic 4: retail, day, zero day, zero, insider, insider zero, middle, man middle, man, retail unknown\n",
            "2026-01-27 15:57:51,840 INFO Topic 4: retail, day, zero day, zero, insider, insider zero, middle, man middle, man, retail unknown\n",
            "2026-01-27 15:57:51,840 INFO Topic 4: retail, day, zero day, zero, insider, insider zero, middle, man middle, man, retail unknown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Topic 4: retail, day, zero day, zero, insider, insider zero, middle, man middle, man, retail unknown\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,852 INFO Topic 5: software, unpatched, unpatched software, telecommunications, insider unpatched, unknown, unknown unpatched, insider, group unpatched, ransomware\n",
            "2026-01-27 15:57:51,852 INFO Topic 5: software, unpatched, unpatched software, telecommunications, insider unpatched, unknown, unknown unpatched, insider, group unpatched, ransomware\n",
            "2026-01-27 15:57:51,852 INFO Topic 5: software, unpatched, unpatched software, telecommunications, insider unpatched, unknown, unknown unpatched, insider, group unpatched, ransomware\n",
            "2026-01-27 15:57:51,852 INFO Topic 5: software, unpatched, unpatched software, telecommunications, insider unpatched, unknown, unknown unpatched, insider, group unpatched, ransomware\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Topic 5: software, unpatched, unpatched software, telecommunications, insider unpatched, unknown, unknown unpatched, insider, group unpatched, ransomware\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,871 INFO Topic 6: weak passwords, weak, passwords, insider, unknown weak, insider weak, government, unknown, group weak, ddos\n",
            "2026-01-27 15:57:51,871 INFO Topic 6: weak passwords, weak, passwords, insider, unknown weak, insider weak, government, unknown, group weak, ddos\n",
            "2026-01-27 15:57:51,871 INFO Topic 6: weak passwords, weak, passwords, insider, unknown weak, insider weak, government, unknown, group weak, ddos\n",
            "2026-01-27 15:57:51,871 INFO Topic 6: weak passwords, weak, passwords, insider, unknown weak, insider weak, government, unknown, group weak, ddos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Topic 6: weak passwords, weak, passwords, insider, unknown weak, insider weak, government, unknown, group weak, ddos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,878 INFO Vectorizing for clustering (TF-IDF): max_features=1000, ngram_range=[1, 2]\n",
            "2026-01-27 15:57:51,878 INFO Vectorizing for clustering (TF-IDF): max_features=1000, ngram_range=[1, 2]\n",
            "2026-01-27 15:57:51,878 INFO Vectorizing for clustering (TF-IDF): max_features=1000, ngram_range=[1, 2]\n",
            "2026-01-27 15:57:51,878 INFO Vectorizing for clustering (TF-IDF): max_features=1000, ngram_range=[1, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Vectorizing for clustering (TF-IDF): max_features=1000, ngram_range=[1, 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:51,977 INFO Fitting KMeans with 6 clusters\n",
            "2026-01-27 15:57:51,977 INFO Fitting KMeans with 6 clusters\n",
            "2026-01-27 15:57:51,977 INFO Fitting KMeans with 6 clusters\n",
            "2026-01-27 15:57:51,977 INFO Fitting KMeans with 6 clusters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Fitting KMeans with 6 clusters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,159 INFO \n",
            "--- K-MEANS CLUSTER SIZES ---\n",
            "2026-01-27 15:57:52,159 INFO \n",
            "--- K-MEANS CLUSTER SIZES ---\n",
            "2026-01-27 15:57:52,159 INFO \n",
            "--- K-MEANS CLUSTER SIZES ---\n",
            "2026-01-27 15:57:52,159 INFO \n",
            "--- K-MEANS CLUSTER SIZES ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:\n",
            "--- K-MEANS CLUSTER SIZES ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,169 INFO Cluster 0: 566 rows\n",
            "2026-01-27 15:57:52,169 INFO Cluster 0: 566 rows\n",
            "2026-01-27 15:57:52,169 INFO Cluster 0: 566 rows\n",
            "2026-01-27 15:57:52,169 INFO Cluster 0: 566 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Cluster 0: 566 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,173 INFO Cluster 1: 173 rows\n",
            "2026-01-27 15:57:52,173 INFO Cluster 1: 173 rows\n",
            "2026-01-27 15:57:52,173 INFO Cluster 1: 173 rows\n",
            "2026-01-27 15:57:52,173 INFO Cluster 1: 173 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Cluster 1: 173 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,176 INFO Cluster 2: 513 rows\n",
            "2026-01-27 15:57:52,176 INFO Cluster 2: 513 rows\n",
            "2026-01-27 15:57:52,176 INFO Cluster 2: 513 rows\n",
            "2026-01-27 15:57:52,176 INFO Cluster 2: 513 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Cluster 2: 513 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,180 INFO Cluster 3: 579 rows\n",
            "2026-01-27 15:57:52,180 INFO Cluster 3: 579 rows\n",
            "2026-01-27 15:57:52,180 INFO Cluster 3: 579 rows\n",
            "2026-01-27 15:57:52,180 INFO Cluster 3: 579 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Cluster 3: 579 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,196 INFO Cluster 4: 557 rows\n",
            "2026-01-27 15:57:52,196 INFO Cluster 4: 557 rows\n",
            "2026-01-27 15:57:52,196 INFO Cluster 4: 557 rows\n",
            "2026-01-27 15:57:52,196 INFO Cluster 4: 557 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Cluster 4: 557 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,202 INFO Cluster 5: 612 rows\n",
            "2026-01-27 15:57:52,202 INFO Cluster 5: 612 rows\n",
            "2026-01-27 15:57:52,202 INFO Cluster 5: 612 rows\n",
            "2026-01-27 15:57:52,202 INFO Cluster 5: 612 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Cluster 5: 612 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,208 INFO Saving results to Analysis_Results.csv\n",
            "2026-01-27 15:57:52,208 INFO Saving results to Analysis_Results.csv\n",
            "2026-01-27 15:57:52,208 INFO Saving results to Analysis_Results.csv\n",
            "2026-01-27 15:57:52,208 INFO Saving results to Analysis_Results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Saving results to Analysis_Results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-27 15:57:52,281 INFO Success! Results saved to Analysis_Results.csv\n",
            "2026-01-27 15:57:52,281 INFO Success! Results saved to Analysis_Results.csv\n",
            "2026-01-27 15:57:52,281 INFO Success! Results saved to Analysis_Results.csv\n",
            "2026-01-27 15:57:52,281 INFO Success! Results saved to Analysis_Results.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:analysis:Success! Results saved to Analysis_Results.csv\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "from typing import List\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Optional: use nltk for lemmatization\n",
        "USE_NLTK = True\n",
        "if USE_NLTK:\n",
        "    try:\n",
        "        import nltk\n",
        "        from nltk.corpus import stopwords\n",
        "        from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "        # Ensure required data is available\n",
        "        try:\n",
        "            _ = stopwords.words(\"english\")\n",
        "            _ = WordNetLemmatizer()\n",
        "        except LookupError:\n",
        "            nltk.download(\"punkt\", quiet=True)\n",
        "            nltk.download(\"wordnet\", quiet=True)\n",
        "            nltk.download(\"omw-1.4\", quiet=True)\n",
        "            nltk.download(\"stopwords\", quiet=True)\n",
        "            _ = stopwords.words(\"english\")\n",
        "            _ = WordNetLemmatizer()\n",
        "    except Exception:\n",
        "        USE_NLTK = False\n",
        "\n",
        "# Fallback stopwords\n",
        "from sklearn.feature_extraction import text as sklearn_text\n",
        "\n",
        "DEFAULT_STOPWORDS = set(sklearn_text.ENGLISH_STOP_WORDS)\n",
        "if USE_NLTK:\n",
        "    try:\n",
        "        DEFAULT_STOPWORDS |= set(stopwords.words(\"english\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "logger = logging.getLogger(\"analysis\")\n",
        "logger.setLevel(logging.INFO)\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n",
        "handler.setFormatter(formatter)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "\n",
        "def clean_text(text: str, lemmatize: bool = True) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    # Remove URLs and emails\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
        "    # Remove non letters (keep spaces)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    # Collapse whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # Remove stopwords and optionally lemmatize\n",
        "    tokens = [t for t in text.split() if t not in DEFAULT_STOPWORDS and len(t) > 1]\n",
        "    if lemmatize and USE_NLTK:\n",
        "        try:\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "        except Exception:\n",
        "            pass\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def build_corpus(df: pd.DataFrame, text_columns: List[str]) -> pd.Series:\n",
        "    present_cols = [c for c in text_columns if c in df.columns]\n",
        "    if not present_cols:\n",
        "        raise ValueError(f\"None of the specified text columns are present. Available columns: {df.columns.tolist()}\")\n",
        "    # Fill NaN with empty strings\n",
        "    df_loc = df.copy()\n",
        "    df_loc[present_cols] = df_loc[present_cols].fillna(\"\")\n",
        "    corpus = df_loc[present_cols].astype(str).agg(\" \".join, axis=1)\n",
        "    return corpus\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    if not os.path.isfile(args.input):\n",
        "        logger.error(\"Input file not found: %s\", args.input)\n",
        "        sys.exit(1)\n",
        "\n",
        "    logger.info(\"Loading CSV: %s\", args.input)\n",
        "    df = pd.read_csv(args.input, low_memory=False)\n",
        "\n",
        "    logger.info(\"Building corpus from columns: %s\", args.text_columns)\n",
        "    try:\n",
        "        corpus_raw = build_corpus(df, args.text_columns)\n",
        "    except ValueError as e:\n",
        "        logger.error(str(e))\n",
        "        sys.exit(1)\n",
        "\n",
        "    logger.info(\"Cleaning text (lemmatize=%s)...\", args.lemmatize)\n",
        "    corpus_clean = corpus_raw.apply(lambda t: clean_text(t, lemmatize=args.lemmatize))\n",
        "    non_empty_mask = corpus_clean.str.strip().astype(bool)\n",
        "    if non_empty_mask.sum() == 0:\n",
        "        logger.error(\"No non-empty text after cleaning. Exiting.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    df = df.loc[non_empty_mask].copy()\n",
        "    df[\"clean_corpus\"] = corpus_clean.loc[non_empty_mask].values\n",
        "\n",
        "    # Vectorization for LDA: CountVectorizer\n",
        "    logger.info(\"Vectorizing for LDA: max_features=%d, ngram_range=%s\", args.count_features, args.ngram_range)\n",
        "    count_vect = CountVectorizer(max_features=args.count_features, ngram_range=tuple(args.ngram_range))\n",
        "    counts = count_vect.fit_transform(df[\"clean_corpus\"])\n",
        "\n",
        "    # Fit LDA\n",
        "    logger.info(\"Fitting LDA with %d topics\", args.lda_topics)\n",
        "    lda = LatentDirichletAllocation(n_components=args.lda_topics, random_state=42, learning_method=\"batch\", max_iter=10)\n",
        "    lda.fit(counts)\n",
        "\n",
        "    # Show top words per topic\n",
        "    feature_names = count_vect.get_feature_names_out()\n",
        "    logger.info(\"--- TOP WORDS PER LDA TOPIC ---\")\n",
        "    n_top_words = args.top_words\n",
        "    for idx, topic in enumerate(lda.components_):\n",
        "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
        "        top_words = [feature_names[i] for i in top_indices]\n",
        "        logger.info(\"Topic %d: %s\", idx + 1, \", \".join(top_words))\n",
        "\n",
        "    # Vectorization for clustering: TF-IDF\n",
        "    logger.info(\"Vectorizing for clustering (TF-IDF): max_features=%d, ngram_range=%s\", args.tfidf_features, args.ngram_range)\n",
        "    tfidf = TfidfVectorizer(max_features=args.tfidf_features, ngram_range=tuple(args.ngram_range), stop_words=None)\n",
        "    tfidf_matrix = tfidf.fit_transform(df[\"clean_corpus\"])\n",
        "\n",
        "    # KMeans clustering\n",
        "    logger.info(\"Fitting KMeans with %d clusters\", args.clusters)\n",
        "    kmeans = KMeans(n_clusters=args.clusters, random_state=42, n_init=10)\n",
        "    df[\"cluster\"] = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "    logger.info(\"\\n--- K-MEANS CLUSTER SIZES ---\")\n",
        "    cluster_counts = df[\"cluster\"].value_counts().sort_index()\n",
        "    for c, cnt in cluster_counts.items():\n",
        "        logger.info(\"Cluster %s: %d rows\", c, cnt)\n",
        "\n",
        "    # Save results\n",
        "    logger.info(\"Saving results to %s\", args.output)\n",
        "    df.to_csv(args.output, index=False)\n",
        "\n",
        "    # Optionally save models/vectorizers\n",
        "    if args.save_models:\n",
        "        base = os.path.splitext(args.output)[0]\n",
        "        logger.info(\"Saving vectorizers and models to %s_*.joblib\", base)\n",
        "        joblib.dump(count_vect, base + \"_count_vectorizer.joblib\")\n",
        "        joblib.dump(tfidf, base + \"_tfidf_vectorizer.joblib\")\n",
        "        joblib.dump(lda, base + \"_lda_model.joblib\")\n",
        "        joblib.dump(kmeans, base + \"_kmeans_model.joblib\")\n",
        "\n",
        "    logger.info(\"Success! Results saved to %s\", args.output)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"NLP analysis for cybersecurity dataset\")\n",
        "    parser.add_argument(\"--input\", \"-i\", required=True, help=\"Input CSV file path\")\n",
        "    parser.add_argument(\n",
        "        \"--text-columns\",\n",
        "        \"-c\",\n",
        "        nargs=\"?\",\n",
        "        default=[\"Attack Type\", \"Target Industry\", \"Attack Source\", \"Security Vulnerability Type\"],\n",
        "        help=\"Columns to combine into the corpus (default: 4 columns used previously)\",\n",
        "    )\n",
        "    parser.add_argument(\"--output\", \"-o\", default=\"Analysis_Results.csv\", help=\"Output CSV path\")\n",
        "    parser.add_argument(\"--tfidf-features\", type=int, default=1000, help=\"Max features for TF-IDF vectorizer\")\n",
        "    parser.add_argument(\"--count-features\", type=int, default=1000, help=\"Max features for CountVectorizer (LDA)\")\n",
        "    parser.add_argument(\"--lda-topics\", type=int, default=6, help=\"Number of LDA topics\")\n",
        "    parser.add_argument(\"--clusters\", type=int, default=6, help=\"Number of KMeans clusters\")\n",
        "    parser.add_argument(\"--top-words\", type=int, default=10, help=\"Top words to display per topic\")\n",
        "    parser.add_argument(\"--save-models\", action=\"store_true\", help=\"Save vectorizers and models as joblib files\")\n",
        "    parser.add_argument(\"--lemmatize\", action=\"store_true\", help=\"Apply lemmatization (requires NLTK)\")\n",
        "    parser.add_argument(\"--ngram-range\", nargs=2, type=int, default=[1, 2], help=\"Ngram range (two ints, e.g. 1 2)\")\n",
        "\n",
        "    # For Colab execution, provide mock arguments to argparse\n",
        "    args = parser.parse_args([\"--input\", \"Global_Cybersecurity_Threats_2015-2024 (1).csv\"])\n",
        "\n",
        "    main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "a9943981",
        "outputId": "d3d17923-ac8e-4184-a96b-c618740e8e42"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "analysis_df = pd.read_csv('Analysis_Results.csv')\n",
        "display(analysis_df.head())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Country  Year        Attack Type     Target Industry  \\\n",
              "0    China  2019           Phishing           Education   \n",
              "1    China  2019         Ransomware              Retail   \n",
              "2    India  2017  Man-in-the-Middle                  IT   \n",
              "3       UK  2024         Ransomware  Telecommunications   \n",
              "4  Germany  2018  Man-in-the-Middle                  IT   \n",
              "\n",
              "   Financial Loss (in Million $)  Number of Affected Users Attack Source  \\\n",
              "0                          80.53                    773169  Hacker Group   \n",
              "1                          62.19                    295961  Hacker Group   \n",
              "2                          38.65                    605895  Hacker Group   \n",
              "3                          41.44                    659320  Nation-state   \n",
              "4                          74.41                    810682       Insider   \n",
              "\n",
              "  Security Vulnerability Type Defense Mechanism Used  \\\n",
              "0          Unpatched Software                    VPN   \n",
              "1          Unpatched Software               Firewall   \n",
              "2              Weak Passwords                    VPN   \n",
              "3          Social Engineering     AI-based Detection   \n",
              "4          Social Engineering                    VPN   \n",
              "\n",
              "   Incident Resolution Time (in Hours)  \\\n",
              "0                                   63   \n",
              "1                                   71   \n",
              "2                                   20   \n",
              "3                                    7   \n",
              "4                                   68   \n",
              "\n",
              "                                        clean_corpus  cluster  \n",
              "0  phishing education hacker group unpatched soft...        2  \n",
              "1  ransomware retail hacker group unpatched software        2  \n",
              "2             man middle hacker group weak passwords        2  \n",
              "3  ransomware telecommunications nation state soc...        4  \n",
              "4              man middle insider social engineering        4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8f1a2e5a-bdc6-4b19-9830-dd23278ea78c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>Year</th>\n",
              "      <th>Attack Type</th>\n",
              "      <th>Target Industry</th>\n",
              "      <th>Financial Loss (in Million $)</th>\n",
              "      <th>Number of Affected Users</th>\n",
              "      <th>Attack Source</th>\n",
              "      <th>Security Vulnerability Type</th>\n",
              "      <th>Defense Mechanism Used</th>\n",
              "      <th>Incident Resolution Time (in Hours)</th>\n",
              "      <th>clean_corpus</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>China</td>\n",
              "      <td>2019</td>\n",
              "      <td>Phishing</td>\n",
              "      <td>Education</td>\n",
              "      <td>80.53</td>\n",
              "      <td>773169</td>\n",
              "      <td>Hacker Group</td>\n",
              "      <td>Unpatched Software</td>\n",
              "      <td>VPN</td>\n",
              "      <td>63</td>\n",
              "      <td>phishing education hacker group unpatched soft...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>China</td>\n",
              "      <td>2019</td>\n",
              "      <td>Ransomware</td>\n",
              "      <td>Retail</td>\n",
              "      <td>62.19</td>\n",
              "      <td>295961</td>\n",
              "      <td>Hacker Group</td>\n",
              "      <td>Unpatched Software</td>\n",
              "      <td>Firewall</td>\n",
              "      <td>71</td>\n",
              "      <td>ransomware retail hacker group unpatched software</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>India</td>\n",
              "      <td>2017</td>\n",
              "      <td>Man-in-the-Middle</td>\n",
              "      <td>IT</td>\n",
              "      <td>38.65</td>\n",
              "      <td>605895</td>\n",
              "      <td>Hacker Group</td>\n",
              "      <td>Weak Passwords</td>\n",
              "      <td>VPN</td>\n",
              "      <td>20</td>\n",
              "      <td>man middle hacker group weak passwords</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>UK</td>\n",
              "      <td>2024</td>\n",
              "      <td>Ransomware</td>\n",
              "      <td>Telecommunications</td>\n",
              "      <td>41.44</td>\n",
              "      <td>659320</td>\n",
              "      <td>Nation-state</td>\n",
              "      <td>Social Engineering</td>\n",
              "      <td>AI-based Detection</td>\n",
              "      <td>7</td>\n",
              "      <td>ransomware telecommunications nation state soc...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Germany</td>\n",
              "      <td>2018</td>\n",
              "      <td>Man-in-the-Middle</td>\n",
              "      <td>IT</td>\n",
              "      <td>74.41</td>\n",
              "      <td>810682</td>\n",
              "      <td>Insider</td>\n",
              "      <td>Social Engineering</td>\n",
              "      <td>VPN</td>\n",
              "      <td>68</td>\n",
              "      <td>man middle insider social engineering</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8f1a2e5a-bdc6-4b19-9830-dd23278ea78c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8f1a2e5a-bdc6-4b19-9830-dd23278ea78c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8f1a2e5a-bdc6-4b19-9830-dd23278ea78c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(analysis_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Country\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"India\",\n          \"Germany\",\n          \"China\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2017,\n        \"max\": 2024,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2017,\n          2018,\n          2019\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Attack Type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Phishing\",\n          \"Ransomware\",\n          \"Man-in-the-Middle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target Industry\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Retail\",\n          \"Telecommunications\",\n          \"Education\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Financial Loss (in Million $)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.925350194910532,\n        \"min\": 38.65,\n        \"max\": 80.53,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          62.19,\n          74.41,\n          38.65\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Number of Affected Users\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 203814,\n        \"min\": 295961,\n        \"max\": 810682,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          295961,\n          810682,\n          605895\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Attack Source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Hacker Group\",\n          \"Nation-state\",\n          \"Insider\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Security Vulnerability Type\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Unpatched Software\",\n          \"Weak Passwords\",\n          \"Social Engineering\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Defense Mechanism Used\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"VPN\",\n          \"Firewall\",\n          \"AI-based Detection\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Incident Resolution Time (in Hours)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 7,\n        \"max\": 71,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          71,\n          68,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_corpus\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ransomware retail hacker group unpatched software\",\n          \"man middle insider social engineering\",\n          \"man middle hacker group weak passwords\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cluster\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}